# Security

## K8S Security Primitives

## Authentication

Who can access the cluster?
* Files - username and passwords
* Files - username and tokens
* certificates
* Exernal Auth providers - LDAP
* Service Accounts

**Users x Service Accounts**

Kubernetes does not handle users natively, it requires an external identity management for that.

- **Users**

All user access is managed by kube-apiserver (curl or kubectl).  
Kube-apiserver auths before processing it:
* username/pass in a file !! **NOT RECOMMENDED** !! **DEPRECATED IN v1.19**
  * create a list of users and passwords in a csv file (password,username,userid). A 4th column with group can exist in the csv.
  * kube-apiserver has this file definition in ```--basic-auth-file=user-details.csv```
    * username/token in a file !! **NOT RECOMMENTED** !! **DEPRECATED IN v1.19**
  * instead of user/password, we can have a csv file with tokens (token,user,uid,group)
  * in the kube-apiserver, we need to specify ```--token-auth-file=user-details.csv```
* certificates
* identity services

## TLS Certificates for Cluster Components

All communication between the components is secured using TLS encryption

Certificate is there to guarantee trust between two parts during a transaction.
* **Symmetric Encryption**: send the encrypted data alongside the key. The same key that encrypts, decrypts the content.
* **Asymmetric Encryption**: private key and public keys. 
    * ssh is an example (id_rsa and id_rsa.pub)
        * ~/.ssh/authorized_keys with the public key (lock, in the analogy of key/lock) there.

As for naming conventions:
* public key certificates
    * *.crt or *.pem
* private key certificates
    * *.key or *-key.pem

In the cluster, all servers should hold server certificates and all the clients should hold client certificates.  
**SERVER COMPONENTS**
* **kube-apiserver** should have ```apiserver.crt``` and ```apiserver.key```
* **etcd-server** should have ```etcd.crt``` and ```etcd.key```
* **kubelet-server** should have ```kubelet.crt``` and ```kubelet.key```

**CLIENT COMPONENTS**  
* **kubectl** used by an admin should provide the ```admin.crt``` and the ```admin.key```
* **scheduler** should provide the ```scheduler.crt``` and the ```scheduler.key```, its a client that interacts with the kube-apiserver
* **kube-controll-ermanager** used by an admin should provide the ```controller-manager.crt``` and the ```controller-manager.key```
* **kube-proxy** used by an admin should provide the ```kube-proxy.crt``` and the ```kube-proxy.key```
* **kube-apiserver** can use the ```apiserver.crt``` and ```apiserver.key``` to comm to etcd or generate new ones for this purpose. We can say that the kube-apiserver is a client when it needs to interact with etcd (a server). Also, kube-apiserver needs to talk to the kubelets, hence for the kubelets kube-apiserver is a client, this also requires a set of keys, we can call them ```apiserver-kubelet-client.crt``` and ```apiserver-kubelet-client.key```

To generate the certificates for a cluster, kubernetes requires a CA (Certificate Authority). In fact, we can have 2 CA's, one for the cluster components and one for etcd alone. This means that the CA also has its own set of keys, we can call them ```ca.crt``` and ```ca.key```

**# Generate CA certs and key**
* ```openssl genrsa -out ca.key 2048```
* ```openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr```
* ```openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt```

**# Generate admin certs and keys signed by the CA**
* ```openssl genrsa -out admin.key 2048```
* ```openssl req -new -key admin.key -subj "/CN=kube-admin" -out admin.csr```
* ```openssl x509 -req -in admin.csr -CAcreateserial -CA ca.crt -CAkey ca.key -out admin.crt```

**# Generate kube-apiserver certificates**  
There is a difference in here, since everyone talks to kube-apiserver, and some components may request different namings, we have to add something to the certificate creation  
* ```openssl genrsa -out apiserver.key 2048```
* ```openssl req -new -key apiserver.key -subj "/CN=kube-apiserver" -out apiserver.csr -config openssl.cnf```
* ```cnf
    openssl.cnf

    [req]
    req_extension = v3_req
    [ v3_req ]
    basicConstraints = CA:FALSE
    keyUsage = nonRepudiation,
    subjectAltName = @alt_names
    [alt_names]
    DNS.1 = kubernetes
    DNS.2 = kubernetes.default
    DNS.3 = kubernetes.default.svc
    DNS.4 = kubernetes.default.svc.cluster.local
    IP.1 = 10.96.0.1
    IP.2 = 172.17.0.87
    ```
* ```openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out apiserver.crt```


**# kubelet nodes** certificates should have the CN with the node name

For kubernetes to be able to auth those certs, the different k8s components are required to have the ca root cert.  
Be aware that kuberentes has a CERTIFICATES API, there are requests objects in kubernetes ```kind: CertificateSigningRequest```. This is managed by te controller-manager components.

## Secure Persistent Key Value Store

...

## Authorization

What can they do?
* RBAC Auth
    * ```Role Base Access Control```
    * role is created and user is associated to it
* ABAC Auth
    * ```Attribute Based Authorization```
    * policy in json file, given to apiserver. For each user, add this policy and restart kube-apiserver
    * difficult to manage
* Node Auth
    * ```Node Authorizer```
    * Kubelet certificates should have _system:node_ in the CN
    * any request coming from system:node is authorized by the ```Node Authorizer```
* Webhook Mode
    * Outsource the auth mechanism
    * ```Open Policy Agent``` 

There is also

* AlwaysAllow 
* AlwaysDeny

Those  are set in the kube-apiserver definition with the flag ```--authorization-node=Node,RBAC,Webhook```, in this order.

**RBAC Controls**

Role manifest:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
  namespace: non-default-ns
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "get", "create", "update", "delete"]
  resourceNames: ["blue-pod"]
- apiGroups: [""]
  resources: ["ConfigMap"]
  verbs: ["list", "get", update"]
```

Then, to link the user to the role

Rolebinding manifest:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
```

To check if I have access:

```kubectl auth can-i delete nodes``` or even ```kubectl auth can-i update cm --as dev-user```, maybe even give the ```--namespace``` at the end.

Also, we have ```ClusterRoleBindings``` and ```ClusterRoles```

## Security Contexts

Kubernetes accept few container security settings (container or pod level):

Pod manifest:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
spec:
  securityContext:
    runAsUser: 1000
    capabilities:
      add: ["MAC_ADMIN"]
containers:
- name: ubuntu
  image: ubunut
  command: ["sleep", "260"]
```

## Network Policies

How applications interact with each other in the cluster  
This is "attached" to pods, just like a NSG in Azure.

To attach it, the NetworkSelector uses podSelector.matchlabels.role: db

We can define ingress and egress

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingres:
  - from:
    - podSelector:
        matchlabels:
          name: api-pod
    ports:
    - protocol: TCP
      port: 3306
```

## Kube Config file

Inside the kubeconfig, the following is expected:

```yaml
apiVersion: v1
kind: Config

current-context: admin-playground

clusters:
- name: playground
  clusters:
    # certificate-authority: /etc/path/here/there/ca.cert
    # Instead of certificate-authority, we can have certificate-authority-data;
    certificate-authority-data: cat .crt | base64 | tr -d "\n"
    server: https...

contexts:
- name: admin-playground
  context:
    cluster: playground
    user: kube-admin
    namespace: finance

users:
- name: kube-admin
  user:
    client-certificate: /etc/lala/lala/admin.crt
    client-key: /heeh/heeh/admin.key
```

```kubectl config view``` to list the kubeConfig file  
```kubectl config use-context  prod-user@production``` to change context (user and cluster)
