# Scheduling

## **NodeSelector**

Simpler and easier method

Pod manifest:
```yaml
...
spec:
  nodeSelector:
    size: Large
```

where size=Large is the node label

```bash
$ kubectl label nodes node1 size=Large
```

NodeSelector has limitations. It was 1 to 1. What if we want something like 'Any node that is not small' ?

We solve that with Node Affinity

## **Node Affinity**

Ensure that Pods are scheduled on particular nodes.

Pod manifest:
```yaml
...
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - Large
            - Medium
```

where key=size is the label key and values is a list of all the accepted values.

Operator could also be **NotIn**, also **Exists** if a lable exists, whatever the value, so no values needed for this case. There are more values we can use.

There are 2 types of node affinity labels:
* **requiredDuringSchedulingIgnoredDuringExecution:**
During Scheduling = state where a pod does not exist and is created for the first time.  
If the node would not have the label and we used this rule, the pod will not be scheduled.  
During Execution = pod is running and a change affects node affinity. Pod would continue to run, since Ignored is used

* **preferredDuringSchedulingIgnoredDuringExecution:**
During Scheduling = In case a matching node is not found, the scheduler will ignore node affinity rules. This means that workload is more important than.  
During Execution = pod is running in the node already. Since its ignored, pod will remain running.

* **PLANNED/NEW: ...RequiredDuringExecution:** Required execution would be evicted/terminated if the label of node Affinity is missing.

___

**OBS:** Be aware that most likely, the best solution is a combinarion of Taints & Tolerations + Node Affinity.

If we only use Taints and Tollerations, _special_ pods can still be scheduled to nodes without taints that they tolerate.  
If we only use Node Affinity in those _special_ pods, the other common pods can still use the resources of the nodes designed for those pods.  
By combining those scheduling methods, pods with tolerations and tainted nods will have a weak match but since we also define node affinity, _special_ pods will only be scheduled there and other pods cannot be scheduled in those nodes.