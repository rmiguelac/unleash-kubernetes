# Scheduling and DaemonSets

Scheduling is the process of assigning Pods to nodes so kubeletes can run them.

Scheduler takes into consideration the following:
- Resource requests vs. available node resources
- Various configurations with node labels

### **spec.nodeSelector.my-label**

used in Pods to configure which node(s) it should be scheduled.  
This will tell scheduler that this pod should be scheduled in nodes with this label

### **spec.nodeName**

bypass scheduling and put a Pod directly in that node.o

## **# DaemonSets**

Daemonsets automatically runs a copy of a Pod in each node.  
If new nodes are added to the cluster, a daemonSet will create a pod in that node.  
Scheduling rules > daemonSets. This means that if a pod cannot be scheduled in a node due to label/non-affinity or whatever other reason, daemonSet will be unable to run a pod in that node.  

## **# Labels, Selectors and stuff like that**

In a Pod:

```yaml
...
metadata:
  labels:
    key: value
```

In a ReplicaSet:

```yaml
...
metadata:
  labels:
    rslabel: rslabel-value
spec:
  template:
    metadata:
      labels:
        podlabel: podlabel-value
```

To filter objects using labels:

```bash
$ kubetl get pods -l key1=value,key2=value2
```

#### **Tains and Tolerations**

Pod to node relationship

* **Taint:** Works by marking a label with something that should prevent pods from being scheduled there.  

```bash
$ kubectl taint node node_name key=value:taint-effect
```

for example

```bash
$ kubectl taint nodes node1 app=blue:NoSchedule
```

where taint effect is the behavior of the pod if they would **NOT** tolerate the taint

In a Pod manifest:

```yaml
...
spec:
  ...
  tolerations:
  - key: "app"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"
```
**OBS:** Those values in the manifest _MUST_ be encoded in double quotes

* **Tolerations:** Even tough nodes can have taints and some pods won't be able to be scheduled there, pods can can also have _tolerations_, meaning that they can tolerate a taint and be scheduled in that node.

**Taint Effects**
* **NoSchdule**: pods will not be scheduler
* **PreferNoSchedule**: system will try to avoid the pod in the node
* **NoExecute**: New pods will not be scheduled in the node, pods in the node will be evicted if they do not tolerate the node

_Tains and tolerations are only meant to restrict nodes from accepting certain pods_, which means that the Pod can still be scheduled in another node, even if it tolerates the taint.
## **NodeSelector**

Simpler and easier method

Pod manifest:
```yaml
...
spec:
  nodeSelector:
    size: Large
```

where size=Large is the node label

```bash
$ kubectl label nodes node1 size=Large
```

NodeSelector has limitations. It was 1 to 1. What if we want something like 'Any node that is not small' ?

We solve that with Node Affinity

## **Node Affinity**

Ensure that Pods are scheduled on particular nodes.

Pod manifest:
```yaml
...
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - Large
            - Medium
```

where key=size is the label key and values is a list of all the accepted values.

Operator could also be **NotIn**, also **Exists** if a lable exists, whatever the value, so no values needed for this case. There are more values we can use.

There are 2 types of node affinity labels:
* **requiredDuringSchedulingIgnoredDuringExecution:**
During Scheduling = state where a pod does not exist and is created for the first time.  
If the node would not have the label and we used this rule, the pod will not be scheduled.  
During Execution = pod is running and a change affects node affinity. Pod would continue to run, since Ignored is used

* **preferredDuringSchedulingIgnoredDuringExecution:**
During Scheduling = In case a matching node is not found, the scheduler will ignore node affinity rules. This means that workload is more important than.  
During Execution = pod is running in the node already. Since its ignored, pod will remain running.

* **PLANNED/NEW: ...RequiredDuringExecution:** Required execution would be evicted/terminated if the label of node Affinity is missing.

___

**OBS:** Be aware that most likely, the best solution is a combinarion of Taints & Tolerations + Node Affinity.

If we only use Taints and Tollerations, _special_ pods can still be scheduled to nodes without taints that they tolerate.  
If we only use Node Affinity in those _special_ pods, the other common pods can still use the resources of the nodes designed for those pods.  
By combining those scheduling methods, pods with tolerations and tainted nods will have a weak match but since we also define node affinity, _special_ pods will only be scheduled there and other pods cannot be scheduled in those nodes.
